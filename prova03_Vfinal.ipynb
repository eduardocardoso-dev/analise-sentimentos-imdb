{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8f9737d5",
        "outputId": "203a9314-ef71-4dc8-ccb8-9659d52f5f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#importações\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "#o que é\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Download the missing resource\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jYZtt0O2J-Qh",
        "outputId": "f96e8e1d-96d4-4d69-c031-e11d94003092"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-3662837322.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#carregando e visualisando o dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nCorpo do dataset (linhas/colunas): {df.shape}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'"
          ]
        }
      ],
      "source": [
        "#carregando e visualisando o dataset\n",
        "df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
        "print(f\"\\nCorpo do dataset (linhas/colunas): {df.shape}\\n\")\n",
        "df.head(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XXxRjRR0XMm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ_rCIaBLaeM"
      },
      "outputs": [],
      "source": [
        "#Analise exploratoria\n",
        "df.info()\n",
        "df.describe()\n",
        "print()\n",
        "\n",
        "\"\"\"\n",
        "Nenhuma linha faltando\n",
        "Sem valores nulos\n",
        "object = strings, rótulos 'positive' ou 'negative', nenhum tipo numérico — dados de texto (NLP) #PESQUISAR\n",
        "50000 avaliações (49582 avaliaçoes unicas e 418 duplicadas)\n",
        "top = texto mais repetido, (5 vezes)\n",
        "freq = 25.000 positivos (balanceado)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmBb64hQQcxh"
      },
      "outputs": [],
      "source": [
        "#limpeza e processamento\n",
        "\n",
        "df = df.drop_duplicates() # remover as duplicadas\n",
        "print(f\"Após remoção de duplicatas: {df.shape}\")\n",
        "\n",
        "# Particao para teste DEPOIS EXCLUIR\n",
        "df = df.sample(2000, random_state=50)\n",
        "\n",
        "\n",
        "stop = set(stopwords.words('english')) #conjunto com todas as palavras irrelevantes (stopwords) em inglês como \"the\", \"is\", \"and\" etc.\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "#função que faz o processamento do texto\n",
        "def processar(text):\n",
        "    text = text.lower() #normalizar o texto em minuscula\n",
        "    text = re.sub(r\"<.*?>\", \"\", text) #remover tags html\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text) #remover pontuações\n",
        "    tokens = word_tokenize(text) #tokenizar o texto em palavras\n",
        "    tokens = [t for t in tokens if t not in stop and t.isalpha()] #lista de tokens filtrados\n",
        "    tokens = [lem.lemmatize(t) for t in tokens] #Lematiza cada token\n",
        "    return \" \".join(tokens) #junta e retorna uma unica string\n",
        "\n",
        "# Aplicar ao dataset\n",
        "df['clean_review'] = df['review'].apply(processar)\n",
        "#compara o antes e depois da limpeza\n",
        "df[['review', 'clean_review']].head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz3LFc9aaRi4"
      },
      "outputs": [],
      "source": [
        "#VETORIZAÇÃO\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(df['clean_review'], df['sentiment'], test_size=0.3, random_state=50) #treinamento(70/30)\n",
        "\n",
        "vetorizar = TfidfVectorizer() #vetorizar com  TF-IDF, mais peso as palavras frequentes no documento\n",
        "X_treino_vet = vetorizar.fit_transform(X_treino)\n",
        "X_teste_vet = vetorizar.transform(X_teste)\n",
        "\n",
        "#transforma os rotulos em 0 ou 1\n",
        "y_treino = y_treino.map({'positive': 1, 'negative': 0})\n",
        "y_teste = y_teste.map({'positive': 1, 'negative': 0})\n",
        "\n",
        "\n",
        "\"\"\"amostra da vetorização\n",
        "print(X_treino_vet)\n",
        "print(X_teste_vet)\"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNXaSejdD5T8"
      },
      "outputs": [],
      "source": [
        "#graficos e metricas\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#grafico curva roc\n",
        "def plotar_curva_roc(modelo, X_teste, y_teste, nome_modelo):\n",
        "    # Usa o método correto para obter probabilidades\n",
        "    if hasattr(modelo, \"predict_proba\"):\n",
        "        y_proba = modelo.predict_proba(X_teste)[:, 1]\n",
        "    elif hasattr(modelo, \"prever_prob\"):\n",
        "        y_proba = modelo.prever_prob(X_teste)\n",
        "    else:\n",
        "        raise AttributeError(\"O modelo não possui um método de probabilidade conhecido.\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_teste, y_proba)\n",
        "    auc = roc_auc_score(y_teste, y_proba)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "    plt.xlabel(\"Falsos Positivos (FPR)\")\n",
        "    plt.ylabel(\"Verdadeiros Positivos (TPR)\")\n",
        "    plt.title(f\"Curva ROC - {nome_modelo}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "#grafico matriz de confusao\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plotar_matriz_confusao(y_true, y_pred, nome_modelo):\n",
        "    matriz = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(matriz, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Negativo', 'Positivo'],\n",
        "                yticklabels=['Negativo', 'Positivo'])\n",
        "    plt.title(f\"Matriz de Confusão - {nome_modelo}\")\n",
        "    plt.xlabel('Previsto')\n",
        "    plt.ylabel('Real')\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CFxiwd24c6S"
      },
      "outputs": [],
      "source": [
        "#modelo arvore e svm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix #metricas de avaliação\n",
        "\n",
        "# Treinar árvore de decisão\n",
        "modelo_arv = DecisionTreeClassifier() #modelo\n",
        "modelo_arv.fit(X_treino_vet, y_treino) #aprende com os dados de treino\n",
        "pred_arv = modelo_arv.predict(X_teste_vet) #prever\n",
        "\n",
        "# Treinar SVM\n",
        "svm = SVC(probability=True)\n",
        "svm.fit(X_treino_vet, y_treino)\n",
        "pred_svm = svm.predict(X_teste_vet)\n",
        "\n",
        "# Avaliação\n",
        "def avaliar_modelo(nome, y_true, y_pred):\n",
        "    print(f\"\\n--- {nome} ---\")\n",
        "    print(f\"Acurácia: {accuracy_score(y_true, y_pred):.2%}\")\n",
        "    print(f\"Precisão: {precision_score(y_true, y_pred):.2%}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred):.2%}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred):.2%}\")\n",
        "    print()\n",
        "\n",
        "avaliar_modelo(\"Árvore de Decisão\", y_teste, pred_arv)\n",
        "avaliar_modelo(\"SVM\", y_teste, pred_svm)\n",
        "plotar_matriz_confusao(y_teste, pred_arv, \"Árvore de Decisão\")\n",
        "plotar_curva_roc(modelo_arv, X_teste_vet, y_teste, \"Árvore de Decisão\")\n",
        "print()\n",
        "plotar_matriz_confusao(y_teste, pred_svm, \"SVM\")\n",
        "plotar_curva_roc(svm, X_teste_vet, y_teste, \"SVM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT71KyOAKK5t"
      },
      "outputs": [],
      "source": [
        "#MODELO REDE NEURAL\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Funções\n",
        "#saida 0 ou 1\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def derivada_sigmoid(a):\n",
        "   return a * (1 - a)\n",
        "\n",
        "# Rede Neural 1 camada oculta\n",
        "class RedeMLP:\n",
        "    #tres camadas(entrada,ocukta,saida)\n",
        "    def __init__(self, n_atrb, n_neuronio, taxa_aprend=0.01, epocas=5000,\n",
        "                 random_state=42):\n",
        "        rng = np.random.default_rng(random_state)\n",
        "\n",
        "        self.pesosO = rng.normal(0, np.sqrt(2/n_atrb), size=(n_atrb, n_neuronio))\n",
        "        self.biasO = np.zeros(n_neuronio)\n",
        "        self.pesosS = rng.normal(0, np.sqrt(2/n_neuronio), size=(n_neuronio, 1))\n",
        "        self.biasS = np.zeros(1)\n",
        "        self.lr = taxa_aprend\n",
        "        self.epocas = epocas\n",
        "\n",
        "    #propagation\n",
        "    def _forward(self, X):\n",
        "        z1 = X @ self.pesosO + self.biasO\n",
        "        a1 = sigmoid(z1)    #ativar camada oculta\n",
        "        z2 = a1 @ self.pesosS + self.biasS\n",
        "        a2 = sigmoid(z2)    # ativar camada saída\n",
        "        return z1, a1, z2, a2\n",
        "\n",
        "    #Backpropagation\n",
        "    #x=entrada, y=rotulos\n",
        "    def _backward(self, X, y, z1, a1, a2):\n",
        "        m = X.shape[0]\n",
        "        erro_saida = a2.squeeze() - y       # usar para corrigir os pesos da última camada.\n",
        "        grad_pesosS = (a1.T @ erro_saida[:, None]) / m\n",
        "        grad_biasS = np.mean(erro_saida)\n",
        "        erro_oculta = (erro_saida[:, None] @ self.pesosS.T) * derivada_sigmoid(a1) #como o erro da saida afeta a camda oculta\n",
        "        grad_pesosO = (X.T @ erro_oculta) / m\n",
        "        grad_biasO = np.mean(erro_oculta, axis=0)\n",
        "        # Atualização\n",
        "        self.pesosO -= self.lr * grad_pesosO\n",
        "        self.biasO -= self.lr * grad_biasO\n",
        "        self.pesosS -= self.lr * grad_pesosS\n",
        "        self.biasS -= self.lr * grad_biasS\n",
        "\n",
        "    def treinar(self, X, y):\n",
        "        for _ in range(self.epocas):\n",
        "            z1, a1, z2, a2 = self._forward(X)\n",
        "            self._backward(X, y, z1, a1, a2)\n",
        "\n",
        "    def prever_prob(self, X):\n",
        "        _, _, _, a2 = self._forward(X)\n",
        "        return a2.ravel()\n",
        "\n",
        "    def prever(self, X, thresh=0.5):\n",
        "        return (self.prever_prob(X) >= thresh).astype(int)\n",
        "\n",
        "\n",
        "X = vetorizar.transform(df['clean_review']).toarray()\n",
        "y = df['sentiment'].map({'positive': 1, 'negative': 0}).values\n",
        "\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_treino = scaler.fit_transform(X_treino)\n",
        "X_teste = scaler.transform(X_teste)\n",
        "#Treinamento\n",
        "config_oculta = [1]\n",
        "for n_neuronio in config_oculta:\n",
        "    rede = RedeMLP(n_atrb=X.shape[1], n_neuronio=n_neuronio, taxa_aprend=0.01, epocas=5000)\n",
        "    rede.treinar(X_treino, y_treino)\n",
        "\n",
        "    y_pred = rede.prever(X_teste)\n",
        "    acuracia = np.mean(y_pred == y_teste)\n",
        "    print(f\"Camada oculta: {n_neuronio:>2} neurônios → Acurácia: {acuracia:.2%}\")\n",
        "\n",
        "avaliar_modelo(\"Rede Neural (1 camada oculta)\", y_teste, y_pred)\n",
        "plotar_matriz_confusao(y_teste, y_pred, \"Rede Neural (1 camada oculta)\")\n",
        "print()\n",
        "plotar_curva_roc(rede, X_teste, y_teste, \"Rede Neural (1 camada oculta)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6zeZkpAgiRS"
      },
      "outputs": [],
      "source": [
        "#modelo KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Treinamento\n",
        "modelo_knn = KNeighborsClassifier(n_neighbors=55)\n",
        "modelo_knn.fit(X_treino_vet, y_treino)\n",
        "\n",
        "# Previsão\n",
        "y_pred_knn = modelo_knn.predict(X_teste_vet)\n",
        "\n",
        "# Avaliação\n",
        "avaliar_modelo(\"KNN (k=5)\", y_teste, y_pred_knn)\n",
        "plotar_matriz_confusao(y_teste, y_pred_knn,\"Matriz de Confusão - KNN\")\n",
        "print()\n",
        "plotar_curva_roc(modelo_knn, X_teste_vet, y_teste, \"Curva ROC - KNN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YMhdtTSmlvKl"
      },
      "outputs": [],
      "source": [
        "#kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=50)\n",
        "clusters = kmeans.fit_predict(X_teste_vet)\n",
        "\n",
        "print(\"\\nTabela cruzada (cluster x rótulo real):\")\n",
        "display(pd.crosstab(clusters, y_teste))\n",
        "\n",
        "#avaliaçoes e grafico\n",
        "# Silhouette Score mede o quão bem separados estão os clusters\n",
        "score = silhouette_score(X_teste_vet, clusters)\n",
        "print(f\"\\nSilhouette Score do K-Means: {score:.4f}\")\n",
        "\n",
        "clusters_mapeados = np.where(clusters == 0, 0, 1)\n",
        "avaliar_modelo(\"Modelo Kmeans\", y_teste, clusters_mapeados)\n",
        "plotar_matriz_confusao(y_teste, clusters_mapeados,\"Matriz de Confusão - Kmeans\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpPygL9dCoJn"
      },
      "outputs": [],
      "source": [
        "# Escolher o melhor modelo\n",
        "resultados_modelos = {}\n",
        "\n",
        "def registrar_resultado(nome, y_true, y_pred, proba=None):\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, proba if proba is not None else y_pred)\n",
        "\n",
        "    resultados_modelos[nome] = {\n",
        "        'Acurácia': acc,\n",
        "        'Precisão': prec,\n",
        "        'Recall': rec,\n",
        "        'F1-Score': f1,\n",
        "        'AUC': auc\n",
        "    }\n",
        "# Árvore\n",
        "registrar_resultado(\"Árvore de Decisão\", y_teste, pred_arv, modelo_arv.predict_proba(X_teste_vet)[:, 1])\n",
        "\n",
        "# SVM\n",
        "registrar_resultado(\"SVM\", y_teste, pred_svm, svm.predict_proba(X_teste_vet)[:, 1])\n",
        "\n",
        "# Rede Neural\n",
        "registrar_resultado(\"Rede Neural\", y_teste, y_pred, rede.prever_prob(X_teste))\n",
        "\n",
        "# KNN\n",
        "registrar_resultado(\"KNN\", y_teste, y_pred_knn, modelo_knn.predict_proba(X_teste_vet)[:, 1])\n",
        "\n",
        "# K-Means\n",
        "registrar_resultado(\"K-Means\", y_teste, clusters_mapeados)\n",
        "\n",
        "# Agente que escolhe o melhor modelo\n",
        "def escolher_melhor_modelo(metricas=[\"F1-Score\",\"Acurácia\", \"Precisão\",\"AUC\"]):\n",
        "    print(\"\\n Ranking dos Modelos:\")\n",
        "    df_resultados = pd.DataFrame(resultados_modelos).T\n",
        "    print(df_resultados.sort_values(by=metricas, ascending=False))\n",
        "\n",
        "    melhor = df_resultados.sort_values(by=metricas, ascending=False).index[0]\n",
        "    print(f\"\\n Melhor modelo de acordo com {', '.join(metricas)}: {melhor}\")\n",
        "    return melhor\n",
        "melhor_modelo = escolher_melhor_modelo(metricas=[\"F1-Score\",\"Acurácia\", \"Precisão\",\"AUC\"])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}